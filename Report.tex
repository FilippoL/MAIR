\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{dirtree}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{multicol}

\usepackage[margin=1in]{geometry}
% Code
\usepackage{xcolor}
\usepackage{listings}
 \lstset{
   language=c,
   basicstyle=\scriptsize\ttfamily,
   keywordstyle=\bfseries\ttfamily\color{orange},
   stringstyle=\color{green!50!black}\ttfamily,
   commentstyle=\color{gray}\ttfamily,
   emph={square}, 
   emphstyle=\color{blue}\texttt,
   emph={[2]root,base},
   emphstyle={[2]\color{yac}\texttt},
   showstringspaces=false,
   flexiblecolumns=false,
   tabsize=2,
   numbers=left,
   numberstyle=\tiny,
   numberblanklines=false,
   stepnumber=1,
   numbersep=10pt,
   xleftmargin=15pt
 }
 
\usepackage[backend=biber, style=numeric, autolang]{biblatex}
	\defbibheading{secbib}[]{%
		\section*{#1}%
		\markboth{#1}{#1}}
	\bibliography{bibfile}
    \renewcommand*{\bibfont}{\footnotesize}
    \setlength{\bibitemsep}{0pt plus 0.3ex}
    
\usepackage{subcaption}

\usepackage{fourier} 
\usepackage{array}
\usepackage{makecell}

\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\captionsetup[subfigure]{list=true, font=large, labelfont=bf, 
labelformat=brace, position=top}

\title{Project 1b: Text Classification}
\author{Emiel Scheffer, Filippo Maria Libardi, Igor Kuczuk Modenezi, Jacqueline Isabel Wagner}
\begin{document}
\maketitle
\section{Quantitative Evaluation}
Evaluation on test set containing $223$ samples:
\vspace*{15px}
\\
\begin{tabular}{c|M{3.5cm}|M{3.5cm}|M{3.5cm}}
    & \multicolumn{2}{c}{baseline classifiers} &  \\ 
    & keyword classifier & random classifier & machine learning classifier\\ \toprule
    accuracy & $80.2 \%$ & $22.8 \% & $98.5\\ \hline
    top $3$ categories & \makecell{ack ($100 \%$) \\ deny ($100 \%$)\\ negate ($100 \%$)\\ restart ($100 \%$)\\thank you ($100 \%$)\\ bye($100 \%$)} & \makecell{inform ($39.2 \%$) \\ request ($24.9\%$) \\ thankyou ($16.3\%$)} & \makecell{restart ($100\%$) \\ thankyou ($99.8\%$)  \\  request ($99.5 \%$)}\\ \hline
    worst $3$ categories & \makecell{negate ($39.3 \%$)\\ confirm ($69.2\%)$ \\ affirm ($73.7 \%$)} & \makecell{hello ($0 \%$) \\ ack ($0 \%$) \\ repeat ($0 \%$) \\ reqmore ($0 \%$) \\ restart ($0 \%$) \\} & \makecell{reqmore ($0 \%$) \\ ack ($50 \%$) \\ hello ($72 \%$) \\}  \\ \hline
    average precision & $72.1 \%$ & $7.7 \%$ & $92 \%$ - $99 \%$ \\ \hline
    average recall & $82.3 \%$ & $6.4 \%$ & $84 \%$ - $99 \%$ \\ 
\end{tabular}{} \\

\vspace*{15px}

While it is undoubtedly interesting to know how many cases have been classified correctly overall, employing diverse metrics sheds light on the systems true performance. We decided to calculate the average precision, as well as the average recall, to answer some very relevant questions:
\begin{enumerate}
    \item What proportion of assigned categories was actually correct? (precision)
    \item What proportion of ground truth categories was identified correctly? (recall)
\end{enumerate}{} 
We obtained answers for these questions by (1) calculating the precision and recall for each category and (2) averaging over all possible categories. 

In addition, we chose to highlight the three best performing categories, along with the top three under-performing categories. We obtained the three best categories by analyzing the recall for each category. Using this information we were able to quickly identify commonly incorrectly classified sentence-patterns to improve our systems.

In the above table average precision and recall of the ML classifier states respectively the micro average and macro average.
While macro average computes the metric independently for each single class and then evaluates the average, the micro average will add together each class contribution and then take the average.

In our case is preferable to value more the micro average as the occurrence of each class is not evenly distributed among the test set (e.g. there are 2048 instances of "inform" and 0 of "reqmore"). 

Additionally the evaluation of the ML classifier has been a little more challenging. 
This is due to the numerous different possible ways of classifying text.
The cautious evaluation has also been based on the following parameters:
\begin{multicols}{2}
\begin{enumerate}
    \item Classifier prediction time
    \item Classifier fitting time
    \item Logarithmic loss
    \item Accuracy
    \item Labels pre-processing   
\end{enumerate}
\end{multicols}

Surely point 1 and 2 are hardware-dependent, but nevertheless they make a good performance metric.
Comparison in the following table is between a Random Forest Classifier (the one we ended up using) with 500 trees/estimators, a Feed Forward Neural Network Classifier and a Logistic Regression Classifier.

\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l|l}
     Classifier & RFC & MLPC & LRC \\ \hline
     Prediction Time & 191.8 & 419.5 & 98.6 \\ \hline
     Fitting Time & 4.9 & 6.7 & 4.5 \\ \hline Accuracy & 97.9 & 99.7 & 93.4 \\ \hline Log loss & 0.48 & 55.4 & 1.1
    \end{tabular}
    \caption{comparison of different classifiers}
    \label{table0}
\end{table}{} \\

As shown in the above table the difference between the classifiers is not very large. The real reason why we decided to opt for the RFC is its decent prediction time, good accuracy and discrete logarithmic loss. 

The latter has played a fundamental role in deciding which classifier to use. Log loss heavily penalises classifiers that are confidently incorrect about a classification. 
Linear regression classifier had a very good time performance but scored a very bad log loss, reason why it has been discarded.

During the various approaches we also tried different encoding and preprocessing of our labels. We firstly excluded every stop word in the data set, but this eventually led to a far worst performance (accuracy around 90) and when the classifier was asked to classify a word in run-time, it would most probably classify it as "inform" (even mi-spelled words).

After removing the stop-words exclusion we decided to stem all the labels (reducing inflected words to their word stem), this did not significantly improve any of the metrics, but it did return a smaller data set. 

\section{Error Analysis}
Following the quantitative analysis, we take a more in depth look at the mistakes made by each of the three classifiers. However, since the random classifier randomly assigns categories, there are no sentences which are especially difficult to detect. Therefore we refrain from listing any examples for this specific classifier and instead focus on the keyword classifier in \autoref{keyword} and the machine learning classifier in \autoref{ml}.

\subsection{Keyword Classifier}
\label{keyword}
During our evaluation under-performance was mostly attributed to three different categories: ambiguous categories, overlapping keywords and extremely short messages. 

For some instances the ground truth categorization, while correct, could easily be extended to include additional categories. For clarity, two examples are listed in \autoref{table1}. 
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l}
     sentence & 'what about international food' & 'okay thank you good bye' \\ \hline
     ground truth & reqalts & bye, thankyou \\ \hline
     classification result & reqalts, inform, request & ack, bye, thankyou
    \end{tabular}
    \caption{difficulties related to ambiguous categories}
    \label{table1}
\end{table}{} \\
While 'okay thank you good bye' was correctly classified as corresponding to the categories 'thankyou' and 'bye', it additionally retrieves the category 'ack'. Considering that the sentence contains the phrase 'okay', this categorization seems plausible. 

Furthermore, using certain keywords was necessary to correctly classify large amounts of data belonging to a given category. However, in some instances, these keywords are used in a different context. Hence, resulting in a wrong classification when employing a keyword classifier. An example of this is given in \autoref{table2}.\\
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l}
     sentence & 'no i said Irish' \\ \hline
     ground truth & negate  \\ \hline
     classification result & inform, negate 
    \end{tabular}
    \caption{difficulties related to overlapping keywords}
    \label{table2}
\end{table}{}

Lastly, to prevent overfitting, certain very specific keywords could not be assigned to any category. Thus, extremely short messages only containing highly specific keywords are not classified correctly. Underlining examples are given in \autoref{table3}.
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l|l}
     sentence & 'any' & 'phone' & 'center'\\ \hline
     ground truth & inform & request & inform\\ \hline
     classification result & null & null & null  
    \end{tabular}
    \caption{difficulties stemming from extremely short messages}
    \label{table3}
\end{table}{}

\subsection{Machine Learning Classifier}
\label{ml}
The Machine Learning classifier has different error types than the keyword classifier.
A relevant problem with the classification of utterances by our model is that it often doesn't recognise words belonging to numerous different categories. 
Therefore if a word is present in our dataset but is classified differently almost every time, then the model will not know what to do with it and probably assign the whole sentence to null. 
For instance a sentence composed by three words from an inform utterance and three words from a request one, will most likely result in null. 
An example follows: 

\autoref{table4}. 
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l|l}
     sentence & 'phone number' & 'italian place' & 'i want the number of the italian place' \\ \hline
     ground truth & request & inform & request \\ \hline
     classification result & request & request & null
    \end{tabular}
    \caption{difficulties related to ambiguous categories}
    \label{table4}
\end{table}{} \\

One more significant issue the system encounters is that every word that is not in our data set at all will be recognised as null.
This is not a deficiency by itself but it becomes one as the model predicts the whole sentence to be null. 
This is very significant especially because some very important words are missing in the data set. 
For instance a sentence like "can you tell me more" will be identified as null because tell is not in the knowledge base of the model. While "please say more" is correctly identified as reqmore.

\autoref{table5}. 
\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l|l|l}
     sentence & 'whats the price range again' & 'again please' & 'start again' & '' \\ \hline
     ground truth & request & repeat & restart \\ \hline
     classification result & request & request & null
    \end{tabular}
    \caption{problem related to multi-classified words}
    \label{table5}
\end{table}{} \\

\section{Difficult Cases}

For the comparison, it was used two phrases that were identified as difficult for the systems to classify: an utterance with no information (such as "cough" or "noise", but that doesn't appear in the training data) and an utterance with more than one information. This could cause clashes. 


\begin{table}[h!]
    \centering
    \begin{tabular}{l|l|l|l}
      & keyword classifier & random classifier & machine learning classifier \\ \hline
     '' & & & \\ \hline
     '' & & & \\
    \end{tabular}
    \caption{difficult cases and their results}
    \label{table4}
\end{table}{}

\section{System Comparison}












\end{document}



